{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Détection de Fraude sur les Compteurs Électriques et Gaz\n",
    "\n",
    "## Contexte\n",
    "Une entreprise publique de distribution d'électricité et de gaz a subi d'énormes pertes en raison de manipulations frauduleuses des compteurs par des consommateurs. L'objectif est de détecter et d'identifier les clients impliqués dans des activités frauduleuses en utilisant l'historique de facturation.\n",
    "\n",
    "## Méthodologie\n",
    "\n",
    "### 1. Compréhension des Données\n",
    "- **Données Client**: Informations démographiques et géographiques des clients\n",
    "- **Données Factures**: Historique de consommation et informations sur les compteurs\n",
    "\n",
    "### 2. Approche de Modélisation\n",
    "1. **Exploration des données** (EDA)\n",
    "2. **Ingénierie des features** - Agrégation des factures par client\n",
    "3. **Prétraitement** - Encodage et normalisation\n",
    "4. **Modélisation** - Classification binaire (Fraude vs Non-fraude)\n",
    "5. **Évaluation** - Métriques de performance\n",
    "6. **Prédiction** - Sur l'ensemble de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration de l'affichage\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données\n",
    "print(\"Chargement des données...\")\n",
    "client_train = pd.read_csv('data/client_train.csv')\n",
    "invoice_train = pd.read_csv('data/invoice_train.csv')\n",
    "client_test = pd.read_csv('data/client_test.csv')\n",
    "invoice_test = pd.read_csv('data/invoice_test.csv')\n",
    "\n",
    "print(f\"Client Train: {client_train.shape}\")\n",
    "print(f\"Invoice Train: {invoice_train.shape}\")\n",
    "print(f\"Client Test: {client_test.shape}\")\n",
    "print(f\"Invoice Test: {invoice_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Exploratoire des Données (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aperçu des données client\n",
    "print(\"=== DONNÉES CLIENT ===\")\n",
    "print(client_train.head())\n",
    "print(\"\\nInformations:\")\n",
    "print(client_train.info())\n",
    "print(\"\\nStatistiques descriptives:\")\n",
    "print(client_train.describe())\n",
    "print(\"\\nValeurs manquantes:\")\n",
    "print(client_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution de la variable cible\n",
    "print(\"\\n=== DISTRIBUTION DE LA CIBLE ===\")\n",
    "print(client_train['target'].value_counts())\n",
    "print(f\"\\nProportion de fraude: {client_train['target'].mean():.2%}\")\n",
    "\n",
    "# Visualisation\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "client_train['target'].value_counts().plot(kind='bar', ax=ax[0], title='Distribution de la Cible')\n",
    "ax[0].set_xlabel('Fraude (0=Non, 1=Oui)')\n",
    "ax[0].set_ylabel('Nombre de Clients')\n",
    "client_train['target'].value_counts(normalize=True).plot(kind='pie', ax=ax[1], \n",
    "                                                           autopct='%1.1f%%', \n",
    "                                                           title='Proportion de Fraude')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aperçu des données factures\n",
    "print(\"\\n=== DONNÉES FACTURES ===\")\n",
    "print(invoice_train.head())\n",
    "print(\"\\nInformations:\")\n",
    "print(invoice_train.info())\n",
    "print(\"\\nStatistiques:\")\n",
    "print(invoice_train.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingénierie des Features\n",
    "\n",
    "### Stratégie:\n",
    "Les indicateurs de fraude potentiels incluent:\n",
    "1. **Anomalies de consommation**: Variations inhabituelles, consommation très faible\n",
    "2. **Problèmes de compteur**: Statut du compteur, remarques de lecture\n",
    "3. **Comportement temporel**: Fréquence des factures, patterns de dates\n",
    "4. **Coefficients**: Utilisation de coefficients de correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_invoice_features(invoice_df):\n",
    "    \"\"\"\n",
    "    Créer des features agrégées à partir des données de facturation\n",
    "    \"\"\"\n",
    "    # Copie pour éviter les modifications\n",
    "    df = invoice_df.copy()\n",
    "    \n",
    "    # Conversion de la date\n",
    "    df['invoice_date'] = pd.to_datetime(df['invoice_date'])\n",
    "    \n",
    "    # Calcul de la consommation totale\n",
    "    df['total_consumption'] = (df['consommation_level_1'] + \n",
    "                               df['consommation_level_2'] + \n",
    "                               df['consommation_level_3'] + \n",
    "                               df['consommation_level_4'])\n",
    "    \n",
    "    # Différence d'index\n",
    "    df['index_diff'] = df['new_index'] - df['old_index']\n",
    "    \n",
    "    # Consommation par mois\n",
    "    df['consumption_per_month'] = df['total_consumption'] / (df['months_number'] + 0.01)\n",
    "    df['index_diff_per_month'] = df['index_diff'] / (df['months_number'] + 0.01)\n",
    "    \n",
    "    # Features agrégées par client\n",
    "    agg_features = df.groupby('client_id').agg({\n",
    "        # Statistiques de consommation\n",
    "        'total_consumption': ['mean', 'std', 'min', 'max', 'sum'],\n",
    "        'consumption_per_month': ['mean', 'std', 'min', 'max'],\n",
    "        'index_diff': ['mean', 'std', 'min', 'max'],\n",
    "        'index_diff_per_month': ['mean', 'std'],\n",
    "        \n",
    "        # Statistiques par niveau de consommation\n",
    "        'consommation_level_1': ['mean', 'sum'],\n",
    "        'consommation_level_2': ['mean', 'sum'],\n",
    "        'consommation_level_3': ['mean', 'sum'],\n",
    "        'consommation_level_4': ['mean', 'sum'],\n",
    "        \n",
    "        # Informations sur le compteur\n",
    "        'counter_statue': ['mean', 'std', 'nunique'],\n",
    "        'counter_coefficient': ['mean', 'std', 'max'],\n",
    "        'reading_remarque': ['mean', 'std', 'max'],\n",
    "        'counter_code': ['nunique'],\n",
    "        'counter_number': ['nunique'],\n",
    "        \n",
    "        # Informations temporelles\n",
    "        'months_number': ['mean', 'sum', 'max'],\n",
    "        'invoice_date': ['count', 'min', 'max']\n",
    "    })\n",
    "    \n",
    "    # Aplatir les noms de colonnes\n",
    "    agg_features.columns = ['_'.join(col).strip() for col in agg_features.columns.values]\n",
    "    agg_features.reset_index(inplace=True)\n",
    "    \n",
    "    # Features additionnelles\n",
    "    agg_features['invoice_count'] = agg_features['invoice_date_count']\n",
    "    \n",
    "    # Durée de la relation client (en jours)\n",
    "    agg_features['client_duration_days'] = (\n",
    "        agg_features['invoice_date_max'] - agg_features['invoice_date_min']\n",
    "    ).dt.days\n",
    "    \n",
    "    # Fréquence moyenne des factures (jours entre factures)\n",
    "    agg_features['avg_invoice_frequency'] = (\n",
    "        agg_features['client_duration_days'] / (agg_features['invoice_count'] + 0.01)\n",
    "    )\n",
    "    \n",
    "    # Ratio de consommation nulle (indicateur de fraude potentiel)\n",
    "    zero_consumption = df.groupby('client_id')['total_consumption'].apply(\n",
    "        lambda x: (x == 0).sum() / len(x)\n",
    "    ).reset_index()\n",
    "    zero_consumption.columns = ['client_id', 'zero_consumption_ratio']\n",
    "    agg_features = agg_features.merge(zero_consumption, on='client_id', how='left')\n",
    "    \n",
    "    # Ratio de consommation très faible\n",
    "    low_consumption = df.groupby('client_id')['total_consumption'].apply(\n",
    "        lambda x: (x < x.quantile(0.1)).sum() / len(x)\n",
    "    ).reset_index()\n",
    "    low_consumption.columns = ['client_id', 'low_consumption_ratio']\n",
    "    agg_features = agg_features.merge(low_consumption, on='client_id', how='left')\n",
    "    \n",
    "    # Volatilité de la consommation (indicateur de fraude)\n",
    "    agg_features['consumption_volatility'] = (\n",
    "        agg_features['total_consumption_std'] / (agg_features['total_consumption_mean'] + 0.01)\n",
    "    )\n",
    "    \n",
    "    # Type de compteur majoritaire\n",
    "    counter_type_mode = df.groupby('client_id')['counter_type'].agg(lambda x: x.mode()[0] if len(x.mode()) > 0 else 'UNKNOWN')\n",
    "    counter_type_mode = counter_type_mode.reset_index()\n",
    "    counter_type_mode.columns = ['client_id', 'counter_type_mode']\n",
    "    agg_features = agg_features.merge(counter_type_mode, on='client_id', how='left')\n",
    "    \n",
    "    # Supprimer les colonnes de date\n",
    "    agg_features = agg_features.drop(['invoice_date_min', 'invoice_date_max'], axis=1)\n",
    "    \n",
    "    return agg_features\n",
    "\n",
    "print(\"Création des features d'entraînement...\")\n",
    "train_invoice_features = create_invoice_features(invoice_train)\n",
    "print(f\"Features créées: {train_invoice_features.shape}\")\n",
    "print(\"\\nPremières lignes:\")\n",
    "print(train_invoice_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Création des features de test...\")\n",
    "test_invoice_features = create_invoice_features(invoice_test)\n",
    "print(f\"Features créées: {test_invoice_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusion avec les données client\n",
    "def prepare_client_features(client_df):\n",
    "    \"\"\"\n",
    "    Préparer les features client\n",
    "    \"\"\"\n",
    "    df = client_df.copy()\n",
    "    \n",
    "    # Conversion de la date de création\n",
    "    df['creation_date'] = pd.to_datetime(df['creation_date'], format='%d/%m/%Y')\n",
    "    \n",
    "    # Features temporelles\n",
    "    df['creation_year'] = df['creation_date'].dt.year\n",
    "    df['creation_month'] = df['creation_date'].dt.month\n",
    "    df['client_age_years'] = (datetime.now() - df['creation_date']).dt.days / 365.25\n",
    "    \n",
    "    # Supprimer la colonne date originale\n",
    "    df = df.drop('creation_date', axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Préparation des données client...\")\n",
    "client_train_prep = prepare_client_features(client_train)\n",
    "client_test_prep = prepare_client_features(client_test)\n",
    "\n",
    "print(\"Client train préparé:\")\n",
    "print(client_train_prep.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusion des features\n",
    "print(\"Fusion des features...\")\n",
    "X_train = client_train_prep.merge(train_invoice_features, on='client_id', how='left')\n",
    "X_test = client_test_prep.merge(test_invoice_features, on='client_id', how='left')\n",
    "\n",
    "print(f\"Dataset d'entraînement: {X_train.shape}\")\n",
    "print(f\"Dataset de test: {X_test.shape}\")\n",
    "\n",
    "# Sauvegarder les IDs de test\n",
    "test_ids = X_test['client_id'].copy()\n",
    "\n",
    "# Extraire la cible\n",
    "y_train = X_train['target'].copy()\n",
    "X_train = X_train.drop(['target', 'client_id'], axis=1)\n",
    "X_test = X_test.drop(['client_id'], axis=1)\n",
    "\n",
    "print(f\"\\nFeatures finales: {X_train.shape[1]}\")\n",
    "print(f\"\\nValeurs manquantes dans X_train:\")\n",
    "print(X_train.isnull().sum().sum())\n",
    "print(f\"\\nValeurs manquantes dans X_test:\")\n",
    "print(X_test.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prétraitement des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gestion des valeurs manquantes\n",
    "print(\"Imputation des valeurs manquantes...\")\n",
    "X_train = X_train.fillna(X_train.median())\n",
    "X_test = X_test.fillna(X_train.median())  # Utiliser les médianes du train\n",
    "\n",
    "# Encodage des variables catégorielles\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "print(f\"\\nColonnes catégorielles: {list(categorical_cols)}\")\n",
    "\n",
    "le_dict = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    # Combiner train et test pour assurer la cohérence\n",
    "    combined = pd.concat([X_train[col], X_test[col]], axis=0)\n",
    "    le.fit(combined)\n",
    "    X_train[col] = le.transform(X_train[col])\n",
    "    X_test[col] = le.transform(X_test[col])\n",
    "    le_dict[col] = le\n",
    "\n",
    "print(\"Encodage terminé.\")\n",
    "\n",
    "# Vérification finale\n",
    "print(f\"\\nX_train final: {X_train.shape}\")\n",
    "print(f\"X_test final: {X_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation des features\n",
    "print(\"Normalisation des features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convertir en DataFrame pour garder les noms de colonnes\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "print(\"Normalisation terminée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modélisation\n",
    "\n",
    "### Approche:\n",
    "Nous utilisons un **Gradient Boosting Classifier** qui est particulièrement efficace pour:\n",
    "- Les problèmes de classification binaire\n",
    "- Les données déséquilibrées\n",
    "- La capture de relations non-linéaires complexes\n",
    "- L'importance des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split pour validation\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train_scaled, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_tr.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"\\nDistribution de la cible (training):\")\n",
    "print(y_tr.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle Gradient Boosting\n",
    "print(\"Entraînement du Gradient Boosting Classifier...\")\n",
    "\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    min_samples_split=100,\n",
    "    min_samples_leaf=50,\n",
    "    subsample=0.8,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "gb_model.fit(X_tr, y_tr)\n",
    "print(\"\\nEntraînement terminé!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluation du Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions sur validation\n",
    "y_val_pred = gb_model.predict(X_val)\n",
    "y_val_proba = gb_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Métriques\n",
    "print(\"=== PERFORMANCE SUR VALIDATION ===\")\n",
    "print(\"\\nRapport de classification:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(f\"\\nROC-AUC Score: {roc_auc_score(y_val, y_val_proba):.4f}\")\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Matrice de Confusion')\n",
    "plt.ylabel('Vraie Classe')\n",
    "plt.xlabel('Classe Prédite')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance des features\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': gb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n=== TOP 20 FEATURES LES PLUS IMPORTANTES ===\")\n",
    "print(feature_importance.head(20))\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(20), feature_importance.head(20)['importance'])\n",
    "plt.yticks(range(20), feature_importance.head(20)['feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 20 Features les Plus Importantes')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction sur le Test Set\n",
    "\n",
    "Maintenant que le modèle est entraîné et validé, nous générons les prédictions finales sur l'ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réentraîner sur toutes les données d'entraînement\n",
    "print(\"Réentraînement sur toutes les données d'entraînement...\")\n",
    "final_model = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    min_samples_split=100,\n",
    "    min_samples_leaf=50,\n",
    "    subsample=0.8,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "final_model.fit(X_train_scaled, y_train)\n",
    "print(\"Entraînement final terminé!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions sur le test set\n",
    "print(\"Génération des prédictions sur le test set...\")\n",
    "test_predictions_proba = final_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(f\"Prédictions générées: {len(test_predictions_proba)}\")\n",
    "print(f\"\\nDistribution des probabilités de fraude:\")\n",
    "print(pd.Series(test_predictions_proba).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la distribution des prédictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(test_predictions_proba, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Probabilité de Fraude')\n",
    "plt.ylabel('Nombre de Clients')\n",
    "plt.title('Distribution des Probabilités de Fraude sur le Test Set')\n",
    "plt.axvline(x=0.5, color='r', linestyle='--', label='Seuil de décision (0.5)')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nClients prédits comme frauduleux (prob > 0.5): {(test_predictions_proba > 0.5).sum()}\")\n",
    "print(f\"Proportion: {(test_predictions_proba > 0.5).mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde des Résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du fichier de soumission\n",
    "submission = pd.DataFrame({\n",
    "    'client_id': test_ids,\n",
    "    'target': test_predictions_proba\n",
    "})\n",
    "\n",
    "# Sauvegarder\n",
    "submission.to_csv('SampleSubmission.csv', index=False)\n",
    "print(\"Fichier de soumission sauvegardé: SampleSubmission.csv\")\n",
    "print(f\"\\nPremières lignes:\")\n",
    "print(submission.head(10))\n",
    "print(f\"\\nDernières lignes:\")\n",
    "print(submission.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions et Recommandations\n",
    "\n",
    "### Points Clés de la Solution:\n",
    "\n",
    "1. **Ingénierie des Features**:\n",
    "   - Agrégation des factures par client pour créer un profil de consommation\n",
    "   - Calcul de métriques de volatilité et d'anomalie\n",
    "   - Extraction de patterns temporels\n",
    "\n",
    "2. **Indicateurs de Fraude**:\n",
    "   - Consommation anormalement faible ou nulle\n",
    "   - Forte volatilité de consommation\n",
    "   - Statut de compteur problématique\n",
    "   - Remarques de lecture négatives\n",
    "   - Coefficients de correction élevés\n",
    "\n",
    "3. **Modèle**:\n",
    "   - Gradient Boosting pour capturer les relations complexes\n",
    "   - Prédiction de probabilités pour permettre un ajustement du seuil\n",
    "   - Performance évaluée avec ROC-AUC pour gérer le déséquilibre des classes\n",
    "\n",
    "### Recommandations pour l'Entreprise:\n",
    "\n",
    "1. **Priorisation**: Commencer par les clients avec probabilité > 0.7\n",
    "2. **Inspection**: Vérifier physiquement les compteurs suspects\n",
    "3. **Monitoring**: Suivre les patterns de consommation en temps réel\n",
    "4. **Prévention**: Améliorer la technologie des compteurs (compteurs intelligents)\n",
    "5. **Éducation**: Sensibiliser sur les conséquences légales de la fraude\n",
    "\n",
    "### Améliorations Futures:\n",
    "\n",
    "1. Collecte de plus de données historiques\n",
    "2. Intégration de données géospatiales\n",
    "3. Modèles d'apprentissage profond (Neural Networks)\n",
    "4. Système de détection en temps réel\n",
    "5. Feedback loop avec les résultats des inspections"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
